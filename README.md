# ğŸŒŸ Awesome Mixture-of-Experts [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT)  
ğŸ“š A curated list of **AWESOME** resources about **Mixture-of-Experts (MoE)**, including papers, code, libraries, and more!  
â­ Feel free to **star** and **fork** this repository!

---

## ğŸ“– Contents  
- [ğŸš€ Open Models](#-open-models)  
- [ğŸ“„ Papers](#-papers)  
  - [ğŸ”¥ Must-Read](#-must-read)  
  - [ğŸ“Œ MoE Models](#-moe-models)  
  - [âš™ï¸ MoE Systems](#-moe-systems)  
  - [ğŸŒ MoE Applications](#-moe-applications)  
- [ğŸ› ï¸ Libraries](#-libraries)  

---

## ğŸš€ Open Models  

| Model | Date | Code | Paper |
|--------|------|------|------|
| **OLMoE** | ğŸ“… Sep 2024 | [ğŸ’¾ Repo](https://github.com/allenai/OLMoE) | [ğŸ“œ Paper](https://arxiv.org/abs/2409.02060) |
| **DeepSeekMoE** | ğŸ“… Jan 2024 | [ğŸ’¾ Repo](https://github.com/deepseek-ai/DeepSeek-MoE) | [ğŸ“œ Paper](https://arxiv.org/abs/2401.06066) |
| **LLaMA-MoE** | ğŸ“… Dec 2023 | [ğŸ’¾ Repo](https://github.com/pjlab-sys4nlp/llama-moe) | [ğŸ“œ Paper](https://github.com/pjlab-sys4nlp/llama-moe/blob/main/docs/LLaMA_MoE.pdf) |
| **Mixtral of Experts** | ğŸ“… Dec 2023 | [ğŸ’¾ Repo](https://mistral.ai/news/mixtral-of-experts/) | [ğŸ“œ Paper](https://arxiv.org/abs/2401.04088) |
| **OpenMoE** | ğŸ“… Aug 2023 | [ğŸ’¾ Repo](https://github.com/XueFuzhao/OpenMoE) | [ğŸ“œ Paper](https://arxiv.org/abs/2402.01739) |
| **Efficient Large Scale MoE** | ğŸ“… Dec 2021 | [ğŸ’¾ Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm) | [ğŸ“œ Paper](https://arxiv.org/abs/2112.10684) |
| **Switch Transformers** | ğŸ“… Feb 2021 | [ğŸ’¾ Repo](https://github.com/google-research/t5x/blob/main/docs/models.md) | [ğŸ“œ Paper](https://arxiv.org/abs/2101.03961) |

---

## ğŸ“„ Papers  

### ğŸ”¥ Must-Read  
These are **essential** MoE papers for understanding the field:  
- [ğŸ“œ A Survey on Mixture of Experts(Sep 2022)](https://arxiv.org/abs/2407.06204)
- [ğŸ“œ A Review of Sparse Expert Models in Deep Learning (Sep 2022)](https://arxiv.org/abs/2209.01667)  
- [ğŸ“œ Switch Transformers: Scaling to Trillion Parameter Models (Jan 2021)](https://arxiv.org/abs/2101.03961)  
- [ğŸ“œ GLaM: Efficient Scaling of Language Models with MoE (Dec 2021)](https://arxiv.org/abs/2112.06905)  
- [ğŸ“œ Scaling Vision with Sparse Mixture of Experts (NeurIPS 2021)](https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html)  
- [ğŸ“œ ST-MoE: Designing Stable and Transferable Sparse Expert Models (Feb 2022)](https://arxiv.org/abs/2202.08906)  
- [ğŸ“œ Mixture-of-Experts with Expert Choice Routing (NeurIPS 2022)](https://arxiv.org/abs/2202.09368)  
- [ğŸ“œ Brainformers: Trading Simplicity for Efficiency (ICML 2023)](https://arxiv.org/abs/2306.00008)  
- [ğŸ“œ From Sparse to Soft Mixtures of Experts (Aug 2023)](https://arxiv.org/abs/2308.00951)  

---


### ğŸ“– Publication

- **[ğŸ“œ BAM! Just Like That: Efficient Parameter Upcycling for MoE (NeurIPS 2024)](https://arxiv.org/abs/2408.08274)**  
- **[ğŸ“œ Patch-level Routing in MoE for CNNs (ICML 2023)](https://arxiv.org/abs/2306.04073)**  
- **[ğŸ“œ Robust MoE Training for CNNs (ICCV 2023)](https://arxiv.org/abs/2308.10110v1)**  
- **[ğŸ“œ Merging Experts into One: Efficient MoE (EMNLP 2023)](https://arxiv.org/abs/2310.09832)**  
- **[ğŸ“œ StableMoE: Stable Routing Strategy (ACL 2022)](https://arxiv.org/abs/2204.08396)**  
- **[ğŸ“œ Outrageously Large Neural Networks: The Sparsely-Gated MoE Layer (ICLR 2017)](https://openreview.net/forum?id=B1ckMDqlg)**  
- **[ğŸ“œ Scaling Vision with Sparse MoE (NeurIPS 2021)](https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html)**  
- **[ğŸ“œ BASE Layers: Simplifying Training of Large, Sparse Models (ICML 2021)](https://arxiv.org/abs/2103.16716)**  
- **[ğŸ“œ PAD-Net: An Efficient Framework for Dynamic Networks (ACL 2023)](https://arxiv.org/abs/2211.05528)**  
- **[ğŸ“œ Brainformers: Trading Simplicity for Efficiency (ICML 2023)](https://arxiv.org/abs/2306.00008)**  
- **[ğŸ“œ Go Wider Instead of Deeper (AAAI 2022)](https://arxiv.org/abs/2107.11817)**  
- **[ğŸ“œ Hash Layers for Large Sparse Models (NeurIPS 2021)](https://arxiv.org/abs/2106.04426)**  
- **[ğŸ“œ DSelect-k: Differentiable Selection in MoE for Multi-Task Learning (NeurIPS 2021)](https://arxiv.org/abs/2106.03760)**  
- **[ğŸ“œ CPM-2: Large-scale Cost-effective Pre-trained Language Models (AI Open)](https://www.sciencedirect.com/science/article/pii/S2666651021000310)**  
- **[ğŸ“œ Mixture of Experts: A Literature Survey (Artificial Intelligence Review)](https://link.springer.com/article/10.1007/s10462-012-9338-y)**  
- **[ğŸ“œ LiMoE: Mixture of LiDAR Representation Learners from Automotive Scenes ](https://arxiv.org/pdf/2501.04004)** 

### ğŸ“š arXiv


- **[ğŸ“œ Demystifying the Compression of Mixture-of-Experts Through a Unified Framework (Jun 2024)](https://arxiv.org/abs/2406.02500) [ğŸ”— Repo](https://github.com/DaizeDong/Unified-MoE-Compression)**  
- **[ğŸ“œ Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models (May 2024)](https://arxiv.org/abs/2405.14297) [ğŸ”— Repo](https://github.com/LINs-lab/DynMoE)**  
- **[ğŸ“œ MoEC: Mixture of Expert Clusters (Jul 2022)](https://arxiv.org/abs/2207.09094)**  
- **[ğŸ“œ No Language Left Behind: Scaling Human-Centered Machine Translation (Jul 2022)](https://research.facebook.com/publications/no-language-left-behind/)**  
- **[ğŸ“œ Sparse Fusion MoE are Domain Generalizable Learners (Jun 2022)](https://arxiv.org/abs/2206.04046)**  
- **[ğŸ“œ LIMoE: Language-Image MoE for Multimodal Learning (Jun 2022)](https://arxiv.org/abs/2206.02770)**  
- **[ğŸ“œ Patcher: MoE Transformers for Medical Image Segmentation (Jun 2022)](https://arxiv.org/abs/2206.01741)**  
- **[ğŸ“œ Interpretable MoE for Structured Data (Jun 2022)](https://arxiv.org/abs/2206.02107)**  
- **[ğŸ“œ Task-Specific Expert Pruning in Sparse MoE (Jun 2022)](https://arxiv.org/abs/2206.00277)**  
- **[ğŸ“œ Gating Dropout: Efficient Regularization for MoE Transformers (May 2022)](https://arxiv.org/abs/2205.14336)**  
- **[ğŸ“œ AdaMix: Adapter-based MoE for Efficient Large Language Models (May 2022)](https://arxiv.org/abs/2205.12399)**  
- **[ğŸ“œ Sparse Mixers: Hybrid MoE-BERT Model (May 2022)](https://arxiv.org/abs/2205.12399)**  
- **[ğŸ“œ One Model, Multiple Modalities: Sparsely Activated Approach (May 2022)](https://arxiv.org/abs/2205.06126)**  
- **[ğŸ“œ SkillNet-NLG: General-Purpose Natural Language Generation with MoE (Apr 2022)](https://arxiv.org/abs/2204.12184)**  
- **[ğŸ“œ Residual Mixture of Experts (Apr 2022)](https://arxiv.org/abs/2204.09636)**  
- **[ğŸ“œ Sparsely Activated MoE are Robust Multi-Task Learners (Apr 2022)](https://arxiv.org/abs/2204.07689)**  
- **[ğŸ“œ MoEBERT: Adapting BERT to MoE (Apr 2022)](https://arxiv.org/abs/2204.07675)**  
- **[ğŸ“œ Efficient Language Modeling with Sparse all-MLP (Mar 2022)](https://arxiv.org/abs/2203.06850)**  
- **[ğŸ“œ Parameter-Efficient MoE for Pretrained LMs (Mar 2022)](https://arxiv.org/abs/2203.01104)**  
- **[ğŸ“œ MoE with Expert Choice Routing (Feb 2022)](https://arxiv.org/abs/2101.03961)**  
- **[ğŸ“œ ST-MoE: Designing Stable & Transferable Sparse Experts (Feb 2022)](https://arxiv.org/abs/2202.08906)**  
- **[ğŸ“œ Unified Scaling Laws for Routed LMs (Feb 2022)](https://arxiv.org/abs/2202.01169)**  
- **[ğŸ“œ DeepSpeed & Megatron for Large-scale MoE (Jan 2022)](https://arxiv.org/abs/2201.11990)**  
- **[ğŸ“œ One Student Knows All: From Sparse to Dense MoE (Jan 2022)](https://arxiv.org/abs/2201.10890)**  
- **[ğŸ“œ Dense-to-Sparse Gate for MoE (Dec 2021)](https://arxiv.org/abs/2112.14397)**  
- **[ğŸ“œ Efficient Large-scale LM with MoE (Dec 2021)](https://arxiv.org/abs/2112.10684)**  
- **[ğŸ“œ GLaM: Scaling LMs with MoE (Dec 2021)](https://arxiv.org/abs/2112.06905)**  
- **[ğŸ“œ SpeechMoE2: Mixture-of-Experts Model with Improved Routing (Nov 2021)](https://arxiv.org/abs/2111.11831)**  
- **[ğŸ“œ VLMo: Unified Vision-Language Pre-Training with MoE (Nov 2021)](https://arxiv.org/abs/2111.02358)**  
- **[ğŸ“œ Towards More Effective and Economic Sparsely-Activated Models (Oct 2021)](https://arxiv.org/abs/2110.07431)**  
- **[ğŸ“œ Sparse MoEs meet Efficient Ensembles (Oct 2021)](https://arxiv.org/abs/2110.03360)**  
- **[ğŸ“œ MoEfication: Conditional Computation of Transformer Models for Efficient Inference (Oct 2021)](https://arxiv.org/abs/2110.01786)**  
- **[ğŸ“œ Cross-token Modeling with Conditional Computation (Sep 2021)](https://arxiv.org/abs/2109.02008)**  
- **[ğŸ“œ SpeechMoE: Scaling to Large Acoustic Models with Dynamic Routing MoE (May 2021)](https://arxiv.org/abs/2105.03036)**  
- **[ğŸ“œ Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Jan 2021)](https://arxiv.org/abs/2101.03961)**  
- **[ğŸ“œ Exploring Routing Strategies for Multilingual Mixture-of-Experts Models (Sept 2020)](https://openreview.net/forum?id=ey1XXNzcIZS)**  
---


## âš™ï¸ MoE Systems  
Key papers on **MoE system implementation and optimization**:  

- [ğŸ“œ Pathways: Asynchronous Distributed Dataflow for ML (MLSys 2022)](https://arxiv.org/abs/2203.12533)  
- [ğŸ“œ Alpa: Automating Parallelism for MoE (OSDI 2022)](https://arxiv.org/abs/2201.12023)  
- [ğŸ“œ GShard: Scaling Giant MoE Models (ICLR 2021)](https://openreview.net/forum?id=qrwe7XHTmYb)  
- [ğŸ“œ BaGuaLu: Targeting Brain Scale Pretrained Models with over 37 Million Cores(PPoPP2022)](http://keg.cs.tsinghua.edu.cn/jietang/publications/PPOPP22-Ma%20et%20al.-BaGuaLu%20Targeting%20Brain%20Scale%20Pretrained%20Models%20w.pdf)  
- [ğŸ“œ GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding(ICLR2021)](https://openreview.net/forum?id=qrwe7XHTmYb)
---
### ğŸ“š arXiv

- [ğŸ“œ MegaBlocks: Efficient Sparse Training with MoE (Nov 2022)](https://arxiv.org/abs/2211.15841) 
- [ğŸ“œ HetuMoE: An Efficient Trillion-scale MoE Distributed Training System (Mar 2022)](https://arxiv.org/abs/2203.14685)  
- [ğŸ“œ SE-MoE: Scalable and Efficient MoE Distributed Training & Inference (Mar 2022)](https://arxiv.org/abs/2205.10034)  
- [ğŸ“œ DeepSpeed-MoE: Advancing MoE Inference and Training for Next-Gen AI Scale (Jan 2022)](https://arxiv.org/abs/2201.05596)  
- [ğŸ“œ SWARM Parallelism: Efficient Large Model Training (Sep 2021)](https://openreview.net/forum?id=U1edbV4kNu_)  
- [ğŸ“œ FastMoE: A Fast Mixture-of-Expert Training System (Mar 2021)](https://arxiv.org/abs/2103.13262)
  
## ğŸŒ MoE Applications  
Papers on **real-world applications** of MoE models:  

- [ğŸ“œ No Language Left Behind: Scaling MoE for Machine Translation (Jul 2022)](https://research.facebook.com/publications/no-language-left-behind/)  
- [ğŸ“œ LIMoE: Multimodal Contrastive Learning with MoE (Jun 2022)](https://arxiv.org/abs/2206.02770)  
- [ğŸ“œ Patcher: MoE for Medical Image Segmentation (Jun 2022)](https://arxiv.org/abs/2206.01741)  
- [ğŸ“œ Switch-NeRF: Learning Scene Decomposition with Mixture of Experts for Large-scale Neural Radiance Fields (Feb 2023)](https://openreview.net/forum?id=PQ2zoIZqvm)  
- [ğŸ“œ Spatial Mixture-of-Experts (Nov 2022)](https://arxiv.org/abs/2211.13491)  
- [ğŸ“œ A Mixture-of-Expert Approach to RL-based Dialogue Management (May 2022)](https://arxiv.org/abs/2206.00059)  
- [ğŸ“œ Mixture of Experts for Biomedical Question Answering (Apr 2022)](https://arxiv.org/abs/2204.07469)  

---

## ğŸ› ï¸ Libraries  
Useful **open-source tools** for working with MoE:

| Name | Description | Repo |
|------|------------|------|
| **DeepSpeed MoE** | Microsoftâ€™s library for efficient MoE training | [ğŸ’¾ Repo](https://github.com/microsoft/DeepSpeed) |
| **Fairseq MoE** | Facebookâ€™s MoE implementation | [ğŸ’¾ Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm) |
| **Megatron-LM** | NVIDIAâ€™s large-scale MoE implementation | [ğŸ’¾ Repo](https://github.com/NVIDIA/Megatron-LM) |
| **T5X MoE** | Googleâ€™s MoE variant in T5X | [ğŸ’¾ Repo](https://github.com/google-research/t5x) |
| **Tutel** | Microsoftâ€™s optimized MoE framework for large-scale training | [ğŸ’¾ Repo](https://github.com/microsoft/tutel) |
| **FastMoE** | A high-performance MoE implementation for PyTorch | [ğŸ’¾ Repo](https://github.com/laekov/fastmoe) |
| **Mesh-TensorFlow** | Distributed computation framework for MoE models | [ğŸ’¾ Repo](https://github.com/tensorflow/mesh) |

---

## ğŸŒŸ Contributing  
A large part of this repository is inspired by [XueFuzhao/awesome-mixture-of-experts](https://github.com/XueFuzhao/awesome-mixture-of-experts).  
We greatly appreciate their effort in curating valuable MoE resources!  

ğŸ¯ **Star this repo** if you find it useful! ğŸš€  
