# 🌟 Awesome Mixture-of-Experts [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT)  
📚 A curated list of **AWESOME** resources about **Mixture-of-Experts (MoE)**, including papers, code, libraries, and more!  
⭐ Feel free to **star** and **fork** this repository!

---

## 📖 Contents  
- [🚀 Open Models](#-open-models)  
- [📄 Papers](#-papers)  
  - [🔥 Must-Read](#-must-read)  
  - [📌 MoE Models](#-moe-models)  
  - [⚙️ MoE Systems](#-moe-systems)  
  - [🌎 MoE Applications](#-moe-applications)  
- [🛠️ Libraries](#-libraries)  

---

## 🚀 Open Models  

| Model | Date | Code | Paper |
|--------|------|------|------|
| **OLMoE** | 📅 Sep 2024 | [💾 Repo](https://github.com/allenai/OLMoE) | [📜 Paper](https://arxiv.org/abs/2409.02060) |
| **DeepSeekMoE** | 📅 Jan 2024 | [💾 Repo](https://github.com/deepseek-ai/DeepSeek-MoE) | [📜 Paper](https://arxiv.org/abs/2401.06066) |
| **LLaMA-MoE** | 📅 Dec 2023 | [💾 Repo](https://github.com/pjlab-sys4nlp/llama-moe) | [📜 Paper](https://github.com/pjlab-sys4nlp/llama-moe/blob/main/docs/LLaMA_MoE.pdf) |
| **Mixtral of Experts** | 📅 Dec 2023 | [💾 Repo](https://mistral.ai/news/mixtral-of-experts/) | [📜 Paper](https://arxiv.org/abs/2401.04088) |
| **OpenMoE** | 📅 Aug 2023 | [💾 Repo](https://github.com/XueFuzhao/OpenMoE) | [📜 Paper](https://arxiv.org/abs/2402.01739) |
| **Efficient Large Scale MoE** | 📅 Dec 2021 | [💾 Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm) | [📜 Paper](https://arxiv.org/abs/2112.10684) |
| **Switch Transformers** | 📅 Feb 2021 | [💾 Repo](https://github.com/google-research/t5x/blob/main/docs/models.md) | [📜 Paper](https://arxiv.org/abs/2101.03961) |

---

## 📄 Papers  

### 🔥 Must-Read  
These are **essential** MoE papers for understanding the field:  
- [📜 A Survey on Mixture of Experts(Sep 2022)](https://arxiv.org/abs/2407.06204)
- [📜 A Review of Sparse Expert Models in Deep Learning (Sep 2022)](https://arxiv.org/abs/2209.01667)  
- [📜 Switch Transformers: Scaling to Trillion Parameter Models (Jan 2021)](https://arxiv.org/abs/2101.03961)  
- [📜 GLaM: Efficient Scaling of Language Models with MoE (Dec 2021)](https://arxiv.org/abs/2112.06905)  
- [📜 Scaling Vision with Sparse Mixture of Experts (NeurIPS 2021)](https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html)  
- [📜 ST-MoE: Designing Stable and Transferable Sparse Expert Models (Feb 2022)](https://arxiv.org/abs/2202.08906)  
- [📜 Mixture-of-Experts with Expert Choice Routing (NeurIPS 2022)](https://arxiv.org/abs/2202.09368)  
- [📜 Brainformers: Trading Simplicity for Efficiency (ICML 2023)](https://arxiv.org/abs/2306.00008)  
- [📜 From Sparse to Soft Mixtures of Experts (Aug 2023)](https://arxiv.org/abs/2308.00951)  

---


### 📖 Publication

- **[📜 BAM! Just Like That: Efficient Parameter Upcycling for MoE (NeurIPS 2024)](https://arxiv.org/abs/2408.08274)**  
- **[📜 Patch-level Routing in MoE for CNNs (ICML 2023)](https://arxiv.org/abs/2306.04073)**  
- **[📜 Robust MoE Training for CNNs (ICCV 2023)](https://arxiv.org/abs/2308.10110v1)**  
- **[📜 Merging Experts into One: Efficient MoE (EMNLP 2023)](https://arxiv.org/abs/2310.09832)**  
- **[📜 StableMoE: Stable Routing Strategy (ACL 2022)](https://arxiv.org/abs/2204.08396)**  
- **[📜 Outrageously Large Neural Networks: The Sparsely-Gated MoE Layer (ICLR 2017)](https://openreview.net/forum?id=B1ckMDqlg)**  
- **[📜 Scaling Vision with Sparse MoE (NeurIPS 2021)](https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html)**  
- **[📜 BASE Layers: Simplifying Training of Large, Sparse Models (ICML 2021)](https://arxiv.org/abs/2103.16716)**  
- **[📜 PAD-Net: An Efficient Framework for Dynamic Networks (ACL 2023)](https://arxiv.org/abs/2211.05528)**  
- **[📜 Brainformers: Trading Simplicity for Efficiency (ICML 2023)](https://arxiv.org/abs/2306.00008)**  
- **[📜 Go Wider Instead of Deeper (AAAI 2022)](https://arxiv.org/abs/2107.11817)**  
- **[📜 Hash Layers for Large Sparse Models (NeurIPS 2021)](https://arxiv.org/abs/2106.04426)**  
- **[📜 DSelect-k: Differentiable Selection in MoE for Multi-Task Learning (NeurIPS 2021)](https://arxiv.org/abs/2106.03760)**  
- **[📜 CPM-2: Large-scale Cost-effective Pre-trained Language Models (AI Open)](https://www.sciencedirect.com/science/article/pii/S2666651021000310)**  
- **[📜 Mixture of Experts: A Literature Survey (Artificial Intelligence Review)](https://link.springer.com/article/10.1007/s10462-012-9338-y)**  
- **[📜 LiMoE: Mixture of LiDAR Representation Learners from Automotive Scenes ](https://arxiv.org/pdf/2501.04004)** 

### 📚 arXiv


- **[📜 Demystifying the Compression of Mixture-of-Experts Through a Unified Framework (Jun 2024)](https://arxiv.org/abs/2406.02500) [🔗 Repo](https://github.com/DaizeDong/Unified-MoE-Compression)**  
- **[📜 Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models (May 2024)](https://arxiv.org/abs/2405.14297) [🔗 Repo](https://github.com/LINs-lab/DynMoE)**  
- **[📜 MoEC: Mixture of Expert Clusters (Jul 2022)](https://arxiv.org/abs/2207.09094)**  
- **[📜 No Language Left Behind: Scaling Human-Centered Machine Translation (Jul 2022)](https://research.facebook.com/publications/no-language-left-behind/)**  
- **[📜 Sparse Fusion MoE are Domain Generalizable Learners (Jun 2022)](https://arxiv.org/abs/2206.04046)**  
- **[📜 LIMoE: Language-Image MoE for Multimodal Learning (Jun 2022)](https://arxiv.org/abs/2206.02770)**  
- **[📜 Patcher: MoE Transformers for Medical Image Segmentation (Jun 2022)](https://arxiv.org/abs/2206.01741)**  
- **[📜 Interpretable MoE for Structured Data (Jun 2022)](https://arxiv.org/abs/2206.02107)**  
- **[📜 Task-Specific Expert Pruning in Sparse MoE (Jun 2022)](https://arxiv.org/abs/2206.00277)**  
- **[📜 Gating Dropout: Efficient Regularization for MoE Transformers (May 2022)](https://arxiv.org/abs/2205.14336)**  
- **[📜 AdaMix: Adapter-based MoE for Efficient Large Language Models (May 2022)](https://arxiv.org/abs/2205.12399)**  
- **[📜 Sparse Mixers: Hybrid MoE-BERT Model (May 2022)](https://arxiv.org/abs/2205.12399)**  
- **[📜 One Model, Multiple Modalities: Sparsely Activated Approach (May 2022)](https://arxiv.org/abs/2205.06126)**  
- **[📜 SkillNet-NLG: General-Purpose Natural Language Generation with MoE (Apr 2022)](https://arxiv.org/abs/2204.12184)**  
- **[📜 Residual Mixture of Experts (Apr 2022)](https://arxiv.org/abs/2204.09636)**  
- **[📜 Sparsely Activated MoE are Robust Multi-Task Learners (Apr 2022)](https://arxiv.org/abs/2204.07689)**  
- **[📜 MoEBERT: Adapting BERT to MoE (Apr 2022)](https://arxiv.org/abs/2204.07675)**  
- **[📜 Efficient Language Modeling with Sparse all-MLP (Mar 2022)](https://arxiv.org/abs/2203.06850)**  
- **[📜 Parameter-Efficient MoE for Pretrained LMs (Mar 2022)](https://arxiv.org/abs/2203.01104)**  
- **[📜 MoE with Expert Choice Routing (Feb 2022)](https://arxiv.org/abs/2101.03961)**  
- **[📜 ST-MoE: Designing Stable & Transferable Sparse Experts (Feb 2022)](https://arxiv.org/abs/2202.08906)**  
- **[📜 Unified Scaling Laws for Routed LMs (Feb 2022)](https://arxiv.org/abs/2202.01169)**  
- **[📜 DeepSpeed & Megatron for Large-scale MoE (Jan 2022)](https://arxiv.org/abs/2201.11990)**  
- **[📜 One Student Knows All: From Sparse to Dense MoE (Jan 2022)](https://arxiv.org/abs/2201.10890)**  
- **[📜 Dense-to-Sparse Gate for MoE (Dec 2021)](https://arxiv.org/abs/2112.14397)**  
- **[📜 Efficient Large-scale LM with MoE (Dec 2021)](https://arxiv.org/abs/2112.10684)**  
- **[📜 GLaM: Scaling LMs with MoE (Dec 2021)](https://arxiv.org/abs/2112.06905)**  
- **[📜 SpeechMoE2: Mixture-of-Experts Model with Improved Routing (Nov 2021)](https://arxiv.org/abs/2111.11831)**  
- **[📜 VLMo: Unified Vision-Language Pre-Training with MoE (Nov 2021)](https://arxiv.org/abs/2111.02358)**  
- **[📜 Towards More Effective and Economic Sparsely-Activated Models (Oct 2021)](https://arxiv.org/abs/2110.07431)**  
- **[📜 Sparse MoEs meet Efficient Ensembles (Oct 2021)](https://arxiv.org/abs/2110.03360)**  
- **[📜 MoEfication: Conditional Computation of Transformer Models for Efficient Inference (Oct 2021)](https://arxiv.org/abs/2110.01786)**  
- **[📜 Cross-token Modeling with Conditional Computation (Sep 2021)](https://arxiv.org/abs/2109.02008)**  
- **[📜 SpeechMoE: Scaling to Large Acoustic Models with Dynamic Routing MoE (May 2021)](https://arxiv.org/abs/2105.03036)**  
- **[📜 Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Jan 2021)](https://arxiv.org/abs/2101.03961)**  
- **[📜 Exploring Routing Strategies for Multilingual Mixture-of-Experts Models (Sept 2020)](https://openreview.net/forum?id=ey1XXNzcIZS)**  
---


## ⚙️ MoE Systems  
Key papers on **MoE system implementation and optimization**:  

- [📜 Pathways: Asynchronous Distributed Dataflow for ML (MLSys 2022)](https://arxiv.org/abs/2203.12533)  
- [📜 Alpa: Automating Parallelism for MoE (OSDI 2022)](https://arxiv.org/abs/2201.12023)  
- [📜 GShard: Scaling Giant MoE Models (ICLR 2021)](https://openreview.net/forum?id=qrwe7XHTmYb)  
- [📜 BaGuaLu: Targeting Brain Scale Pretrained Models with over 37 Million Cores(PPoPP2022)](http://keg.cs.tsinghua.edu.cn/jietang/publications/PPOPP22-Ma%20et%20al.-BaGuaLu%20Targeting%20Brain%20Scale%20Pretrained%20Models%20w.pdf)  
- [📜 GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding(ICLR2021)](https://openreview.net/forum?id=qrwe7XHTmYb)
---
### 📚 arXiv

- [📜 MegaBlocks: Efficient Sparse Training with MoE (Nov 2022)](https://arxiv.org/abs/2211.15841) 
- [📜 HetuMoE: An Efficient Trillion-scale MoE Distributed Training System (Mar 2022)](https://arxiv.org/abs/2203.14685)  
- [📜 SE-MoE: Scalable and Efficient MoE Distributed Training & Inference (Mar 2022)](https://arxiv.org/abs/2205.10034)  
- [📜 DeepSpeed-MoE: Advancing MoE Inference and Training for Next-Gen AI Scale (Jan 2022)](https://arxiv.org/abs/2201.05596)  
- [📜 SWARM Parallelism: Efficient Large Model Training (Sep 2021)](https://openreview.net/forum?id=U1edbV4kNu_)  
- [📜 FastMoE: A Fast Mixture-of-Expert Training System (Mar 2021)](https://arxiv.org/abs/2103.13262)
  
## 🌎 MoE Applications  
Papers on **real-world applications** of MoE models:  

- [📜 No Language Left Behind: Scaling MoE for Machine Translation (Jul 2022)](https://research.facebook.com/publications/no-language-left-behind/)  
- [📜 LIMoE: Multimodal Contrastive Learning with MoE (Jun 2022)](https://arxiv.org/abs/2206.02770)  
- [📜 Patcher: MoE for Medical Image Segmentation (Jun 2022)](https://arxiv.org/abs/2206.01741)  
- [📜 Switch-NeRF: Learning Scene Decomposition with Mixture of Experts for Large-scale Neural Radiance Fields (Feb 2023)](https://openreview.net/forum?id=PQ2zoIZqvm)  
- [📜 Spatial Mixture-of-Experts (Nov 2022)](https://arxiv.org/abs/2211.13491)  
- [📜 A Mixture-of-Expert Approach to RL-based Dialogue Management (May 2022)](https://arxiv.org/abs/2206.00059)  
- [📜 Mixture of Experts for Biomedical Question Answering (Apr 2022)](https://arxiv.org/abs/2204.07469)  

---

## 🛠️ Libraries  
Useful **open-source tools** for working with MoE:

| Name | Description | Repo |
|------|------------|------|
| **DeepSpeed MoE** | Microsoft’s library for efficient MoE training | [💾 Repo](https://github.com/microsoft/DeepSpeed) |
| **Fairseq MoE** | Facebook’s MoE implementation | [💾 Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm) |
| **Megatron-LM** | NVIDIA’s large-scale MoE implementation | [💾 Repo](https://github.com/NVIDIA/Megatron-LM) |
| **T5X MoE** | Google’s MoE variant in T5X | [💾 Repo](https://github.com/google-research/t5x) |
| **Tutel** | Microsoft’s optimized MoE framework for large-scale training | [💾 Repo](https://github.com/microsoft/tutel) |
| **FastMoE** | A high-performance MoE implementation for PyTorch | [💾 Repo](https://github.com/laekov/fastmoe) |
| **Mesh-TensorFlow** | Distributed computation framework for MoE models | [💾 Repo](https://github.com/tensorflow/mesh) |

---

## 🌟 Contributing  
A large part of this repository is inspired by [XueFuzhao/awesome-mixture-of-experts](https://github.com/XueFuzhao/awesome-mixture-of-experts).  
We greatly appreciate their effort in curating valuable MoE resources!  

🎯 **Star this repo** if you find it useful! 🚀  
